# -*- coding: utf-8 -*-
"""DA450Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uz2uzY582UDfE3FZx4rxW0EhFwf3_00H
"""

!pip install ydata-profiling

import numpy as np
import os
import pandas as pd
import warnings
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
import plotly.express as px
import plotly.graph_objects as go
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import MinMaxScaler
import missingno as msgn
from sklearn import linear_model
import joblib
import librosa
from sklearn.metrics import *
import argparse
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, Input
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from scipy.interpolate import UnivariateSpline
from matplotlib import pyplot as plt
import ydata_profiling

data = pd.read_csv('/content/voice.csv')

data.head()

profile = ydata_profiling.ProfileReport(data, explorative=True)

profile.to_notebook_iframe()

data.duplicated().sum()

data= data.drop_duplicates()

data.duplicated().sum()

"""**Q25 and Q75 are effective features for distinguishing between male and female voices.**

"I chose Q25 and Q75 because they are robust and less sensitive to outliers, effectively capturing the range and spread of the voice frequencies. They clearly differentiate male and female voices, as seen in the scatter plot."
"""

fig = px.scatter(data, x="Q25", y="Q75", color="label",
                 size='median',hover_data=['label'])
fig.show()

"""Red Points (Female): These points dominate the upper-right area of the chart, meaning:
Q25 and Q75 values are both higher for female voices.
This suggests that female voices tend to have higher frequencies or stronger signal intensities at the analyzed quantiles.
Blue Points (Male): These points cluster more toward the lower-left area, meaning:
Q25 and Q75 values are lower for male voices.
This aligns with the fact that male voices typically occupy a lower frequency range.
"""

dic = {'label':{'male':1,'female':0}}
data.replace(dic,inplace = True)

data['label']

X = data.drop(columns=['label'])
y = data['label']

scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X)

# LSTM requires input in the shape (samples, time steps, features)
# Since we have 21 features, we reshape to (samples, 21, 1)

X_reshaped = X_normalized.reshape(X_normalized.shape[0], X_normalized.shape[1], 1)

x_train, x_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)


print("X_train shape:", x_train.shape)
print("X_test shape:", x_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

model= Sequential([
    Input(shape=(21,1)),
    LSTM(128, return_sequences= True),
    Dropout(0.2),
    LSTM(64, return_sequences= False),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])

model.summary()

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if(logs.get('accuracy')>0.95):
            print("\nReached 95% accuracy so cancelling training!")
            self.model.stop_training = True

callbacks = myCallback()

batch_size = 64
epochs = 100
# train the model using the training set and validating using validation set
model.fit(x_train, y_train, epochs=epochs,
          batch_size=batch_size,
          callbacks=[callbacks])

model.save('model.h5')

model.load_weights('model.h5')

loss, accuracy = model.evaluate(x_test,y_test, verbose=0)
print(f"Loss: {loss:.4f}")
print(f"Accuracy: {accuracy*100:.2f}%")

preds = []

for i in range(0,len(x_test)):
    # Pass a single sample to model.predict using X_test[i:i+1]
    preds.append(model.predict(x_test[i:i+1])[0][0])

predictions = [1 if val >0.5 else 0 for val in preds]

print("Overall Accuracy Score is : {}".format(accuracy_score(y_test, predictions)))

def plot_pdf(y_pred, y_test, name=None, smooth=500):
    positives = y_pred[y_test == 1] # Changed y_test.label to y_test as y_test is a Series, not a DataFrame
    negatives = y_pred[y_test == 0]
    N = positives.shape[0]
    n =10
    s = positives
    p, x = np.histogram(s, bins=n) # bin it into n = N//10 bins
    x = x[:-1] + (x[1] - x[0])/2   # convert bin edges to centers
    f = UnivariateSpline(x, p, s=n)
    plt.plot(x, f(x))

    N = negatives.shape[0]
    n = 10
    s = negatives
    p, x = np.histogram(s, bins=n) # bin it into n = N//10 bins
    x = x[:-1] + (x[1] - x[0])/2   # convert bin edges to centers
    f = UnivariateSpline(x, p, s=n)
    plt.plot(x, f(x))
    plt.xlim([0.0, 1.0])
    plt.xlabel('density')
    plt.ylabel('density')
    plt.title('PDF-{}'.format(name))
    plt.show()

# Assuming 'preds' from the previous cell contains the model predictions
plot_pdf(np.array(preds), y_test, 'Keras') # Pass 'preds' as y_pred_keras and convert to NumPy array

